{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53125333-f5e1-4abf-bee0-a4b632659663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f6af72-4e9e-4fa2-8571-78e47edd9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pushshift.io/reddit/search/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff38a20-2e64-4fe8-b25a-7afd86ca6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d1b7ee-e9e9-4d76-853e-4bf089a0cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts(base_url,subreddit,length):\n",
    "    data_pulls = pd.DataFrame()\n",
    "    before = 1650821963\n",
    "    for i in range(length):\n",
    "        parameters = {'subreddit': subreddit,\n",
    "                  'size': 100,\n",
    "                  'is_self' : True,\n",
    "                  'before': before}\n",
    "        try:\n",
    "            response = requests.get(base_url+'submission/',parameters)\n",
    "            if response.status_code != 200:\n",
    "                print('request failed')\n",
    "            else:\n",
    "                data = response.json()\n",
    "                posts = data['data']\n",
    "                posts = pd.DataFrame(posts)\n",
    "                data_pulls = pd.concat([data_pulls,posts])\n",
    "                time.sleep(5)\n",
    "                before = data_pulls['created_utc'].iloc[-1]\n",
    "        except:\n",
    "            print(\"Exception: {}\".format(type(exception).__name__))\n",
    "            print(\"Exception message: {}\".format(exception))\n",
    "            time.sleep(15)\n",
    "    return data_pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2801b257-d398-42dc-a278-6735281868fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#futurology_pulls = pull_posts(base_url,'futurology',289)\n",
    "#futurology_pulls.to_csv('futurology_pull.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a44b531-0c04-409d-8cdb-a4ca04b238fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_pulls = pull_posts(base_url,'collapse',288)\n",
    "#collapse_pulls.to_csv('collapse_pull.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81337711-04f8-43e2-9491-83523587c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/wgwfmfsx53n97rhpm9z12wqh0000gn/T/ipykernel_9797/2351499472.py:1: DtypeWarning: Columns (0,1,4,6,8,9,10,11,12,13,17,19,20,21,22,23,24,26,29,30,32,33,34,35,36,40,42,44,48,49,54,58,61,66,71,72,75,76,77,81,84,85,86,87,88,89,90,96,97) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_fut = pd.read_csv('futurology_pull.csv')\n",
      "/var/folders/pr/wgwfmfsx53n97rhpm9z12wqh0000gn/T/ipykernel_9797/2351499472.py:2: DtypeWarning: Columns (0,1,8,9,10,11,12,13,17,19,20,21,22,23,24,26,27,29,33,34,35,41,45,49,50,51,59,67,70,72,74,76,78,79,80,81,82,83,87,88,89,90,96,97) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_col = pd.read_csv('collapse_pull.csv')\n"
     ]
    }
   ],
   "source": [
    "df_fut = pd.read_csv('futurology_pull.csv')\n",
    "df_col = pd.read_csv('collapse_pull.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a9006-c2a0-4057-918c-03e595816792",
   "metadata": {},
   "source": [
    "### Combining Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa03e3d-dfd6-49e4-ae46-a420c7183dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fut_cut = df_fut[['subreddit','title']]\n",
    "df_col_cut = df_col[['subreddit','title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84bf9dc-3f0c-4c0c-9392-9b406d516de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.concat([df_col_cut,df_fut_cut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97900f1e-6227-40f6-94c5-45b9564f2add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57617, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2916a-3093-4613-8ba7-df35fa164fcd",
   "metadata": {},
   "source": [
    "### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1da68f0-307a-4bee-b48b-160ab226eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_comb['title']\n",
    "y = df_comb['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284cbdfd-7aac-4d67-b987-f6dd9761e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368e5bf-576c-4b7b-bf49-11acfb7506cd",
   "metadata": {},
   "source": [
    "### Baseline Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c73e96-5d7a-4be6-8c02-7ede78a39e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011454356126345"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = DummyClassifier()\n",
    "dc.fit(X_train,y_train)\n",
    "dc.predict(X_test)\n",
    "dc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892dd00-879f-4124-a910-6c299a76cfbd",
   "metadata": {},
   "source": [
    "### Initial LogReg Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "044e1a0c-bd35-449f-8e01-e1e74b3f7c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemma_tokenizer(doc):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(w) for w in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f0082bc-24cb-4295-8143-e9813f0fa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "    ('cv',CountVectorizer(stop_words='english',ngram_range=[1,2],min_df=3,tokenizer=lemma_tokenizer)), \n",
    "    ('lr',LogisticRegression(max_iter=10000))   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d1b7ee-9824-4159-bede-e47b904bcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielschlant/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9234471905952051, 0.8469281499479347)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "pipe.score(X_train,y_train), pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f2b973-4ba7-4200-b0f7-26f2c781098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(\n",
    "    [\n",
    "    ('tf',TfidfVectorizer(stop_words='english',ngram_range=[1,2],min_df=3,tokenizer=lemma_tokenizer)),\n",
    "    ('lr',LogisticRegression(max_iter=10000))   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d0608ee-b748-4a81-b360-3d679e6ffa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielschlant/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8988938257891327, 0.8469281499479347)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.fit(X_train,y_train)\n",
    "pipe2.score(X_train,y_train), pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd899f5d-7426-4f0b-9e70-60ac40caca01",
   "metadata": {},
   "source": [
    "### Deeper Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ec7a96f-fdd6-4437-8bc4-8420ca73ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = ['?',',','.','(',')',':','!','u','\\'','i','are','that','the','on','and','is','to','a','of',\\\n",
    "               'in','what','for','will','you','it','be','this','with','do','n\\'t','-','\\'s','\\'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8947dd5e-555e-44e8-aa26-37ccd8a39aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielschlant/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words=my_stopwords,ngram_range=[1,2],min_df=3,tokenizer=lemma_tokenizer)\n",
    "cvec.fit(X_train)\n",
    "X_train_cv = cvec.transform(X_train)\n",
    "X_test_cv = cvec.transform(X_test)\n",
    "\n",
    "X_train_cvdf = pd.DataFrame(X_train_cv.A,columns=cvec.get_feature_names_out())\n",
    "X_test_cvdf = pd.DataFrame(X_test_cv.A,columns=cvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82237807-58b5-4e48-b491-5b6e7e0ffcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collapse    4991\n",
       "we          4403\n",
       "how         3743\n",
       "future      3551\n",
       "about       2299\n",
       "have        2241\n",
       "or          2064\n",
       "can         2008\n",
       "would       1941\n",
       "if          1934\n",
       "``          1869\n",
       "think       1845\n",
       "''          1829\n",
       "world       1780\n",
       "from        1740\n",
       "year        1651\n",
       "your        1597\n",
       "people      1589\n",
       "an          1564\n",
       "why         1530\n",
       "climate     1514\n",
       "not         1474\n",
       "’           1469\n",
       "our         1467\n",
       "by          1456\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvdf.sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48a13dc3-a1a8-4d62-a810-7dfe5e7d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(max_iter=10_000)\n",
    "# lr.fit(X_train_cvdf,y_train)\n",
    "# lr.score(X_train_cvdf,y_train),lr.score(X_test_cvdf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09952de5-1a18-44fe-9dec-d0291730a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConfusionMatrixDisplay.from_estimator(lr,X_test_cvdf,y_test,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0128a9-18d1-4f03-9d89-533d4451ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(lr.coef_[0],index=cvec.get_feature_names_out(),columns=['coef_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27786863-1929-4c16-b014-2d3c16c49c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527aa230-f365-42cf-b025-4140f76d8b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67a790-e411-460e-a153-c371b2d9a35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df8af6-309d-4dfe-aece-ae4072428e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294918c-3a72-4ba2-8da1-d3f43de11d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657715c3-320f-42b7-b971-dd827cde0fd4",
   "metadata": {},
   "source": [
    "### What should we be attempting to optimize for?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
